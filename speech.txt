

Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization


Luca Della Libera 1 2 Cem Subakan 3 1 2 Mirco Ravanelli 1 2


Abstract
Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete to- kens that can be processed by LLMs. How- ever, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that en- ables variable-frame-rate tokenization through soft character-level alignment and explicit du- ration modeling. DyCAST learns to associate tokens with character-level linguistic units dur- ing training and supports alignment-free infer- ence with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increas- ing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using signifi- cantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.

1 Introduction
In the past few years, tokenization has emerged as a key building block in multimodal LLMs, enabling joint au- toregressive modeling across modalities (Grattafiori et al., 2024; Jiang et al., 2024a; Comanici et al., 2025; Singh et al., 2025; DeepSeek-AI et al., 2025). By representing diverse modalities as sequences of discrete tokens, these models can operate over text, images, and audio within a shared modeling framework. For speech, tokenization is typically achieved through neural audio codecs, which map continuous waveforms to discrete tokens. Such repre- sentations have driven recent progress in generative speech

1Concordia	University,	Montreal,	Canada	2Mila-
Quebec AI Institute, Montreal, Canada 3Universite´ Laval, Que´bec, Canada.	Correspondence to:  Luca Della Libera
<luca.dellalibera@mail.concordia.ca>.
Preprint. February 5, 2026.

and audio modeling (Borsos et al., 2023; Chen et al., 2025; Nguyen et al., 2025; De´fossez et al., 2024; Labiausse et al., 2025; Zeghidour et al., 2025), enabling LLMs to operate di- rectly on speech. Originally developed for efficient transmis- sion (Zeghidour et al., 2021; De´fossez et al., 2023), modern codecs now produce compact yet powerful representations that support high-quality resynthesis as well as competitive performance in both discriminative and generative down- stream tasks (Mousavi et al., 2025; Guo et al., 2025).
Despite their success, most existing speech tokenizers rely on frame-level representations at a fixed frame rate, pro- ducing tokens at a constant temporal resolution. While effective for reconstruction, this design is poorly matched to the inherently variable temporal structure of speech: silence and steady regions are information-poor, whereas rapidly changing segments are information-dense (Van Kuyk et al., 2017; Dieleman et al., 2021; Cuervo et al., 2022). As a result, fixed-frame-rate tokenization leads to inefficient se- quence lengths and limited alignment with corresponding text, making generative modeling more challenging. Al- though dynamic-frame-rate codecs and text-supervised alignment methods have recently emerged, this area is still developing. Existing approaches often rely on heuris- tic frame merging or clustering strategies that are weakly grounded in linguistic structure (Wang et al., 2025a; Zhang et al., 2025; Zheng et al., 2026; Li et al., 2025b), or re- quire transcriptions or alignment information at inference time (Tseng et al., 2025; Hsu et al., 2025), limiting flexibility in fully speech-based scenarios.
In this work, we introduce DyCAST, a Dynamic Character- Aligned Speech Tokenization framework that enables variable-frame-rate coding through soft character-level alignment. Each speech token is approximately associated with a character in the underlying transcript, with the degree of alignment controlled by a learned boundary predictor. At inference time, this alignment can be adjusted dynami- cally, allowing a flexible trade-off between strict character alignment and longer token spans. As a result, DyCAST can operate either with ground-truth alignments when available (e.g. for text-to-speech) or in a fully alignment-free mode based on the learned soft alignment alone. Additionally, Dy- CAST provides explicit control over token durations during decoding via a duration predictor that learns to invert the pooling operation, predicting appropriate token durations from the context. Because character-level representations












Figure 1. DyCAST architecture. Frame-level representations extracted by a frozen, self-supervised encoder are compressed and dynamically grouped into variable-length chunks, pooled, and quantized into discrete tokens. The decoding stage reverses this process by expanding token-level representations back to frame-level features before waveform reconstruction. Character-level boundaries are provided during training by a frozen aligner.

naturally result in very low frame rates, achieving high- quality resynthesis becomes more challenging. To address this limitation, we further introduce retrieval-augmented decoding as an auxiliary mechanism to improve reconstruc- tion quality by leveraging side information, without increas- ing bitrate.
In summary, our contributions are as follows:
• We propose DyCAST, a novel framework for controllable, character-aligned, variable-frame-rate speech tokeniza- tion through soft character-level alignment and explicit duration modeling.
• We introduce retrieval-augmented decoding as an auxil- iary mechanism to enhance speech resynthesis quality by leveraging side information at inference time.
• We empirically show competitive performance on speech resynthesis and a range of other downstream tasks, while using significantly fewer tokens compared to fixed-frame- rate baselines.
Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.
2 Related Work
Fixed-Frame-Rate Codecs. The dominant approach to speech tokenization discretizes audio at a fixed temporal res- olution using frame-level tokens. Early research on neural speech codecs primarily focused on high-fidelity acoustic reconstruction at medium bitrates (Zeghidour et al., 2021; De´fossez et al., 2023; Kumar et al., 2023). In parallel, a separate line of work explored semantic tokenization by discretizing self-supervised phonetic speech representations to capture linguistic content (Baevski et al., 2020; Hsu et al., 2021; Messica & Adi, 2024; Mousavi et al., 2024b; Chang et al., 2025), at the expense of fine-grained acoustic de- tail. More recently, hybrid codecs have emerged with the goal of balancing semantic and acoustic information. These approaches combine multiple design strategies, including the use of multiple codebooks (Ju et al., 2024; Jiang et al., 2024b; Zheng et al., 2025), dual-encoder architectures (Liu et al., 2024), knowledge distillation (Zhang et al., 2024; De´fossez et al., 2024; Yang et al., 2025; Gong et al., 2025;

Li et al., 2025a), or supervised fine-tuning (Har-Tuv et al., 2025), to improve both reconstruction quality and speech language modeling performance. More recently, a grow- ing trend has shifted toward single-codebook designs that jointly encode semantic and acoustic information. These ap- proaches achieve good reconstruction quality at low bitrates while substantially simplifying downstream modeling (Ji et al., 2025; Bai et al., 2024; Xin et al., 2024; Wu et al., 2025; Ye et al., 2025; Della Libera et al., 2025a;b; Song et al., 2025). Our work departs from fixed-rate tokenization by introducing dynamic, character-aligned speech tokens whose durations adapt to linguistic content, providing more efficient and linguistically grounded representations.
Variable-Frame-Rate Codecs. Dynamically adjusting frame rates based on content complexity has recently at- tracted attention across modalities. In the text domain, sev- eral recent works (Nawrot et al., 2023; Slagle, 2024; Ahia et al., 2024; Pagnoni et al., 2025; Videau et al., 2025; Hwang et al., 2025) explore learned tokenization approaches, in which chunking is learned end-to-end according to a task ob- jective, rather than being fixed a priori. In these methods, to- ken boundaries emerge from the optimization process itself, enabling adaptive representations that better match the struc- ture of the underlying data. A similar trend is emerging in au- dio. Early work by Dieleman et al. (2021) proposed an audio VQ-VAE combined with run-length encoding to enable vari- able frame rates. More recently, CodecSlime (Wang et al., 2025a) introduced a multi-stage pipeline in which a fixed- frame-rate codec is first trained and then temporally similar representations are merged to obtain adaptive frame rates. TFC (Zhang et al., 2025) and VARSTok (Zheng et al., 2026) propose dynamic-frame-rate strategies in which duration is encoded implicitly through the structure of the codebook. FlexiCodec (Li et al., 2025b) further builds on VARSTok by enabling substantially lower frame rates through refined merging procedures, while retaining fine-grained control over the resulting token rate. Unlike prior approaches that rely on heuristic frame merging or require transmitting dura- tions, our method uses learned character-level soft alignment to produce semantically aligned variable-rate tokens with-



out duration side information, while still enabling explicit duration control at decoding time.

Text-Aligned Speech Representations. Recent work has explored text-aligned tokenization as a way to bridge the modality gap for joint speech-text modeling. Methods such as TASTE (Tseng et al., 2025) and TASLA (Hsu et al., 2025) learn discrete speech tokens aligned with text by enforcing transcript-speech alignment through cross-attention-based architectures. Similarly, TaDiCodec (Wang et al., 2025b) incorporates textual information in a text-aware diffusion- based speech codec, where speech-text alignment is intro- duced implicitly through autoregressive conditioning on text rather than being enforced explicitly at the tokenization stage. SSR (Tan et al., 2025) and LST (Lu et al., 2025) pursue a similar goal of speech-text alignment, but are tan- gential to our work, as they operate at the speech language modeling level and do not define a standalone speech tok- enization framework. While effective for joint modeling, these methods typically rely on ground-truth alignments or require textual input at inference time, limiting their applica- bility to text-free generative settings and making them closer to TTS-oriented pipelines than general-purpose speech tok- enizers. In contrast, DyCAST is a fully non-autoregressive, autoencoder-based speech tokenizer that operates end-t-o end without requiring text at inference time.
3 DyCAST
The proposed DyCAST framework (see Figure 1) is inspired

frame-level representations within each chunk into a com- pact chunk-level representation. To provide training su- pervision for chunk boundaries, we employ a frozen, pre- trained character aligner. This aligner takes a wave- form as input and returns character-level durations, and can be conveniently implemented using a CTC-based ASR model (Graves et al., 2006) with characters as the vocab- ulary. The resulting frame-level character boundaries are used to derive target chunk durations that serve as supervi- sion during training. To model the boundary distribution, we adopt a discrete-time hazard model (Singer & Willett, 1993; Ren et al., 2019) rather than a standard frame-wise bi- nary cross-entropy objective. Unlike independent boundary classification, the hazard formulation explicitly models the time to the next boundary, allowing boundary predictions to be temporally dependent and properly normalized over time. This is particularly important in our setting, where chunk durations are the primary quantity of interest and boundaries are sparse relative to the frame rate. Moreover, the hazard model naturally enforces a single boundary per chunk and provides a principled likelihood over variable-length seg- ments, which facilitates stable training and coherent bound- ary decoding. Formally, given frame-level representations
x1:T extracted by the compressor, the boundary predictor estimates a boundary probability ht ? (0, 1) at each frame:
                ht = s(f?(x1:T )t) ,	(1) where f? is neural network operating on the sequence of features. The probability that the next boundary occurs k frames after time t is then given by

by the modular compressor-quantizer-decompressor archi- tecture introduced in (Della Libera et al., 2025a), while extending it with dedicated modules for dynamic pool-


P (T = k | t) =

k-1

i=0


(1 - ht+i)


ht+k.	(2)

ing. Given an input waveform, a frozen, pretrained self- supervised speech encoder first extracts high-dimensional, single-stream acoustic-semantic representations at a fixed frame rate. These features are then projected into a lower- dimensional latent space by a lightweight compressor, re- taining the most informative components of the representa- tion. A chunker module subsequently groups consecutive frames into variable-length chunks, yielding a temporally adaptive chunking of the input. Within each chunk, frame- level features are pooled to form compact chunk-level rep- resentations, which are then discretized by a quantizer, producing a sequence of discrete tokens. A dechunker module reverses the chunking operation by expanding each token-level representation back to frame-level features. A decompressor maps the expanded features back to the orig- inal encoder dimensionality, and a decoder finally recon- structs the time-domain waveform.
3.1 Dynamic Chunking
The chunker module consists of two components: a bound- ary predictor, which identifies semantically meaningful chunk boundaries, and a downsampler, which aggregates

The hazard model is trained by maximizing the likelihood of ground-truth next-boundary offsets extracted from forced alignment.
At inference time, chunk boundaries are decoded directly from the hazard predictions without any textual input. Boundaries can be obtained either greedily, yielding de- terministic chunking, or via sequential sampling to intro- duce variability. In both cases, decoding constraints on the minimum and maximum chunk duration, controlled by the hyperparameters min gap and max gap, respectively, are used to regulate the resulting frame rate. In addition, a hazard threshold th controls boundary emission: higher values of th favor longer chunks (lower frame rates), while lower values produce finer-grained chunks (higher frame rates). Formally, a boundary is emitted at frame t when
ht = th subject to the min gap constraint; if no boundary is emitted for max gap consecutive frames, a boundary is
forced. Given the decoded boundaries, the downsampler produces chunk-level representations by selecting the last frame of each chunk. While alternative strategies such as average pooling per chunk are possible, this design keeps



the operation simple and efficient, and preserves the original compressed representations without blending them across frames, which facilitates adaptation to different frame rates.
3.2 Dechunking
Like the chunker, the dechunker module consists of two components: a duration predictor, which estimates the number of frames associated with each character-aligned discrete token, and an upsampler, which expands each chunk-level representation back to frame-level features. The duration predictor is necessary to recover temporal struc- ture at decoding time, since in general only the token se- quence is transmitted, not the chunk boundaries used to derive it. Supervision for duration prediction comes from the same frozen character aligner used during chunking, which provides target token durations corresponding to char- acter spans. To model the distribution of token durations, we adopt a negative binomial duration model (Zen et al., 2009). Compared to alternatives such as geometric or Pois- son distributions, the negative binomial provides greater flexibility by explicitly modeling over-dispersed count data, which is characteristic of speech durations. In particular,




Pool	
Figure 2. Retrieval-augmented decoding (RAD). Discrete latents are refined via similarity search against a pool of continuous latents prior to waveform reconstruction.
At inference time, the duration model supports two decoding regimes. In the free decoding mode, when the total number of frames is unknown, durations are obtained greedily as
dˆi = dmin + round(µfree),	(8)
yielding a predicted total length Tˆ = i dˆi. In the budget- constrained decoding mode, when the target length T is
known (e.g. speech resynthesis), the predicted free means are renormalized to match the available duration budget,

while a geometric model assumes a memoryless process

free	free   Tfree 

(9)

with a fixed hazard rate and a Poisson model ties the mean and variance, the negative binomial decouples these quan-

µ˜i	= µi

I:j

free ,
j

tities, allowing the model to capture the heavy-tailed and highly variable nature of character-level durations in speech. Formally, given a sequence of discrete tokens c1:N , the dura- tion model maps each token to a positive free mean duration µfree > 0,
i	µfree = softplus(g?(c1:N )i) ,	(3)
where g? denotes a neural network operating on the full sequence of tokens. The actual mean token duration is obtained by enforcing a minimum duration dmin,

followed by deterministic integer rounding that enforces
  i dˆi = T exactly. This hybrid formulation enables ex- plicit duration control at decoding time while remaining
independent of character alignment during inference.
3.3 Retrieval-Augmented Decoding
Since the proposed codec is character-aligned, the resulting frame rate is naturally very low (6 - 18 Hz). This makes accurate waveform reconstruction increasingly challenging,

µi = d


min

+ µfree.	(4)

as fine-grained acoustic details, high-frequency informa-
tion, and speaker-specific characteristics tend to be removed

By default, dmin = 1. The excess target duration
               yi = di - dmin = 0	(5) is modeled using the negative binomial distribution with mean µfree and a global dispersion parameter a > 0 shared

by the quantization bottleneck (Yang et al., 2025; Li et al., 2025b). To mitigate this issue without increasing the bi- trate, we propose retrieval-augmented decoding (RAD), a decoder-side mechanism that improves reconstruction qual- ity by selectively incorporating auxiliary information at

across all tokens,
yi

~ NB(µfree, a).	(6)

decoding time (see Figure 2). RAD builds on the fact that

The duration model is trained by minimizing the negative log-likelihood together with a normalized length regular- ization term,

self-supervised speech encoders trained with masking ob- jectives (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022) produce intermediate representations with strong se-

	  N

µfree - T	 2

mantic structure. In these models, representations that are

Ldur

=
i=1

- log pNB(yi | µfree, a) + ?

  i=1  i	free
Tfree + ?

,

(7)

nearby in feature space tend to correspond to similar lin- guistic content, enabling meaningful retrieval at the latent

where Tfree = T - Ndmin denotes the total number of al- locatable frames after enforcing a minimum duration dmin
per token, ? controls the strength of the length regulariza- tion, and ? is a small constant for numerical stability. This normalized penalty encourages globally consistent pacing while remaining invariant to utterance length.

level via similarity search. This property underlies the ef- fectiveness of nearest-neighbor methods in tasks such as voice conversion (Baas et al., 2023; Della Libera et al., 2025a). RAD leverages this structure by maintaining a pool of continuous latents collected from a diverse set of utterances, speakers, and acoustic conditions. During decoding, this pool can be queried to retrieve similar la-



tent vectors. By analogy to retrieval-augmented generation, where a query embedding retrieves relevant documents from a latent knowledge store, RAD retrieves continuous speech latents from this pool via similarity matching against the discrete latent produced by the quantizer. Since these la- tents are already low-dimensional and compact by design, they are well-suited for efficient nearest-neighbor search and large-scale retrieval. For each quantized latent, if the similarity to its nearest retrieved candidate exceeds a pre- defined threshold t , the discrete latent is replaced with the retrieved continuous latent. By setting a sufficiently high similarity threshold, retrieval can be restricted to candidates that preserve both semantic content and speaker iden- tity. In cases where near-identical speech segments exist in the pool, RAD can recover the original continuous rep- resentation, substantially improving speaker fidelity and fine-grained acoustic detail. Interestingly, the candidate pool can be constructed offline and updated over time to

and duration predictor. The boundary predictor uses a bi- nary classification head, while the duration model predicts a free mean parameter together with a shared, learnable dispersion parameter a, as described in Section 3.2. Impor- tantly, the boundary predictor is trained directly on the 1024- dimensional WavLM features rather than the compressed latents, providing richer context for boundary estimation. In contrast, the duration predictor operates on the pooled, quantized low-dimensional latents.
Quantizer. Della Libera et al. (2025a) employ binary spherical quantization (BSQ) (Zhao et al., 2025), which tightly couples bitrate to latent dimensionality L through an implicit codebook of size |C| = 2L. At the low frame rates considered in this work, this coupling limits
representational expressivity. We therefore generalize this formulation to scalar spherical quantization (SSQ). SSQ preserves the spherical constraint of BSQ while allowing each latent dimension to take one of K scalar levels in

adapt to evolving deployment needs (e.g. privacy, memory
footprint, speed, and/or efficiency).

[- 1
L

, 1 ], enabling more flexible bitrates. Compared to
L


4 Experimental Setup
4.1 Architecture
We now describe the DyCAST architecture used in our experiments, inspired by FocalCodec (Della Libera et al., 2025a). Full hyperparameter specifications are reported in Appendix C.
Character Aligner. We employ MMS (Pratap et al., 2024). Alignment information is extracted via simple argmax CTC alignment on the frame-level log probabili- ties obtained by running the model on the input waveform. To handle silence segments detected by the aligner, we do not discard them; instead, we aggregate them into the subse- quent non-silence token. We find this strategy to work well in practice and leave more refined treatments of silence for future work.
Encoder and Decoder. We use the pretrained WavLM- large1 model (Chen et al., 2022) as the encoder. In particular, we use the representations from the 6th transformer layer, which are semantically rich while retaining fine-grained acoustic information (Della Libera et al., 2025a; Baas et al., 2023). Waveform reconstruction from these features is per- formed using Vocos (Siuzdak, 2024) for efficient decoding.
Compressor and Decompressor. Both modules are im- plemented using focal modulation blocks (Yang et al., 2022; Della Libera et al., 2024), leveraging their demonstrated effectiveness and efficiency for low-bitrate speech cod- ing (Della Libera et al., 2025a).
Boundary and Duration Predictor. We adopt the same efficient focal modulation architecture also for the boundary

1https://github.com/microsoft/unilm/tree/master/wavlm

finite scalar quantization (FSQ) (Mentzer et al., 2024), SSQ explicitly enforces spherical geometry via L2 normalization and includes a factorized entropy regularizer for improved codebook utilization. The resulting implicit codebook has size |C| = KL. In our experiments, we set L = 32 and
K = 4, yielding |C| = 432. To make this high cardinality
tractable in practice, we adopt a factorized representation
in which each token is decomposed into 32 parallel streams, each with a vocabulary size of 4.
4.2 Training
We train DyCAST on LibriTTS (Zen et al., 2019), resampled to 16 kHz. We adopt a multi-stage training strategy for improved stability:
1. Reconstruction. We train the compressor-quantization- decompressor for WavLM features reconstruction using L2 loss, without boundary prediction, or duration mod- eling. Dynamic downsampling and upsampling use the character durations from the character aligner in a teacher- forced manner. In parallel, we train the decoder to map WavLM features back to waveform.
2. Boundary predictor training. We train the boundary predictor to map WavLM features to character boundaries, using character aligner durations as supervision.
3. Adaptation to predicted boundaries. We fine-tune the compressor-quantizer-decompressor to operate on dura- tions predicted by the boundary predictor. To improve robustness, during training we randomly sample among character aligner boundaries and predicted boundaries
greedily decoded with th = 0.5, min gap ? {1, 3, 5}, and no max gap constraint (see Section 3.1).
4. Duration predictor training. Finally, we freeze all the other components and train the duration model to pre- dict chunk durations from pooled quantized latents, using the same boundary sampling strategy as in the previous

step. This procedure yields a robust system that general-

Table 1. Codecs considered in our downstream evaluation.



izes well to different boundary configurations, including
min gap values outside the training range.
Hyperparameters and training details are provided in Ap- pendix C.
4.3 Inference
DyCAST provides substantial flexibility at inference time for both encoding and decoding. For encoding, chunk boundaries can be obtained in two ways:
1. Character aligner. We use boundaries provided by the
 
Frame Bitrate Sample Rate (Hz) (kbps) Rate (kHz)

Codebooks Params

character aligner. This is particularly useful for down- stream applications that benefit from accurate speech-text alignment, such as text-to-speech.
2. Boundary predictor. We greedily decode or sample boundaries from the boundary predictor. By adjusting the decoding hyperparameters, we obtain direct control over the effective frame rate. For example, increasing the min gap parameter enforces a larger minimum gap between boundaries, resulting in fewer, more uniformly spaced tokens.
For decoding, three operating modes are available:
1. Tokens + durations. We transmit discrete tokens to- gether with their original durations and use them as side information during decoding. The effective bitrate is the token bitrate plus the overhead required to transmit one duration per token, assuming a bounded duration range.
2. Tokens + utterance length. We transmit discrete to- kens and a single global utterance length, which incurs negligible overhead. The duration model is then used in budget-constrained decoding mode to sample dura- tions whose total length matches the provided value. This mode is particularly suitable for resynthesis tasks.
3. Tokens only. We transmit only discrete tokens and let the duration model infer the most likely durations. This mode is useful for modeling tasks such as text-to-speech and speech language modeling, where tokens do not come with explicit duration information.
Note that, independently of the chosen setting, durations can always be resampled using the duration model or an external model to introduce natural variability in speaking rate, or manually overridden to stretch or compress indi- vidual characters. Overall, this design offers significantly greater flexibility than traditional fixed-rate codecs, enabling fine-grained control over bitrate, timing, and prosody.
5 Downstream Evaluation
For downstream evaluation, we consider four inference con- figurations: DyCAST-CA, DyCAST-BP1, DyCAST-BP3, and DyCAST-BP5. DyCAST-CA uses boundaries provided by the character aligner, while the remaining variants rely on the boundary predictor with min gap set to 1, 3, and 5, respectively, corresponding to average frame rates of

approximately 14 Hz, 9 Hz, and 6 Hz. Unless otherwise stated, all experiments use the tokens + utterance length decoding mode. We provide analysis of alternative decoding strategies and visualizations in Appendix E.1 and E.2.
To showcase the effectiveness of our framework, we com- pare against a range of fixed-frame-rate baselines (see Ta- ble 1). Additional details about these baselines are provided in Appendix B. We follow the evaluation protocol of Della Libera et al. (2025a), considering speech resynthesis under different conditions, voice conversion, and both discrimina- tive and generative downstream tasks. For generative eval- uation, we focus on text-to-speech in limited-data regimes. Details about the datasets and experimental settings can be found in Appendix A and D.
5.1 Speech Resynthesis and Voice Conversion
We evaluate the speech resynthesis (SR) capabilities of DyCAST on LibriSpeech (Panayotov et al., 2015), MLS (Pratap et al., 2020), VoiceBank (Valentini-Botinhao et al., 2016), and Libri1Mix. Naturalness, intelligibility, and speaker similarity are assessed using UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech, dWER (Wang et al., 2021), and Sim (Della Libera et al., 2025a), respectively. Additional details are provided in Appendix D.1. We further evaluate voice conversion (VC) on VCTK (Yamagishi et al., 2017), following the protocol of Della Libera et al. (2025a), which is described in detail in Appendix D.2.
As shown in Table 2, DyCAST achieves a strong balance between reconstruction quality and token efficiency, operat- ing at substantially lower frame rates than fixed-frame-rate baselines while maintaining competitive performance across all metrics. On LibriSpeech, DyCAST variants consistently reach high naturalness scores and low intelligibility degra- dation, closely matching strong fixed-rate codecs such as FocalCodec and Stable Codec, despite using 3-8x fewer frames. Notably, DyCAST-CA and DyCAST-BP1 achieve dWER values comparable to high-rate baselines, confirm- ing that character-aligned or predicted durations preserve linguistic content even at reduced token rates. For mul- tilingual speech, DyCAST maintains robust performance across languages, with only a moderate increase in dWER as the frame rate is reduced. Importantly, speaker similarity

Table 2. Speech resynthesis (SR) and voice conversion (VC). Best and second-best results are highlighted.

Codec Frame Rate (Hz)
?Bitrate (kbps) ?SR - LibriSpeechSR - MLSSR - VoiceBankSR - Libri1MixVC - VCTKUTMOS?dWERSim ?UTMOS?dWERSim ?DNSMOS?dWERSim ?DNSMOS?dWERSim ?UTMOS?dWER ?Sim ?Reference--4.090.00100.02.840.00100.03.560.00100.03.730.00100.04.090.00100.0EnCodec75.01.501.588.0893.81.3329.6095.52.7628.1687.72.4055.1786.31.2486.5272.2DAC50.01.001.2920.0489.21.2456.0889.12.7263.9079.82.4090.9276.61.25104.0067.2WavLM6-KM50.00.453.756.2090.02.9744.5489.53.0620.6782.92.8736.6085.92.9026.6892.4SpeechTokenizer50.01.002.285.1491.61.5556.3292.02.7434.5182.22.5857.2682.81.4920.3281.2SemantiCodec25.00.652.918.9796.01.8736.2197.73.1331.4690.62.6751.1889.92.02106.0072.8Mimi12.50.693.295.7396.02.0830.9696.73.0128.0087.82.6549.1489.42.40110.0089.7WavTokenizer40.00.483.7811.5595.42.6449.7397.03.0942.1289.82.5370.1086.33.1343.1573.4BigCodec80.01.044.112.5598.52.8615.2499.13.1920.6792.32.7553.2688.31.3199.9668.9Stable Codec25.00.704.324.9794.73.4756.9995.93.3320.3288.82.9143.5290.03.7627.6371.1FocalCodec50.00.654.052.1897.42.9612.5798.33.168.0891.32.9327.8991.63.3821.2792.2DyCAST-CA~14.4~0.923.993.3297.42.8516.8998.33.2714.7091.52.9230.9591.83.1925.9492.0DyCAST-BP1~17.5~1.123.923.6197.62.9016.4698.53.2212.6092.12.9231.3792.03.2624.7792.4DyCAST-BP3~9.0~0.573.954.6697.22.9221.6198.33.2317.9691.42.9136.9691.43.2427.6692.3DyCAST-BP5~6.2~0.403.978.8496.52.9434.0497.73.2226.6289.72.9046.4290.03.2135.5992.2

remains consistently high across all DyCAST variants, indi- cating that variable-rate tokenization does not compromise speaker identity, even in low-rate regimes. Notably, aside from the character aligner, which naturally supports multilin- gual inputs, the codec itself is trained exclusively on English speech, highlighting the strong generalization capability of the learned discrete representations. In noisy conditions, Dy- CAST remains competitive with dedicated speech codecs.

Table 3. Retrieval-augmented decoding for different similarity thresholds t on LibriSpeech resynthesis. Best and second-best results are highlighted within each section.	
 Codec	Configuration	UTMOS ? dWER ? Sim ?

w/o retriever
DyCAST-CA retriever (t = 95)3.99
3.853.32
2.9797.4
97.8retriever (t = 97)3.913.0097.8retriever (t = 99)3.993.3397.4w/o retriever
DyCAST-BP1 retriever (t = 95)3.92
3.803.61
3.4097.6
98.0
While very aggressive compression leads to higher dWER,retriever (t = 97)
retriever (t = 99)3.84
3.923.36
3.7798.0
97.6naturalness and speaker similarity remain stable, suggest-w/o retriever3.954.6697.2ing that explicit duration modeling provides robustness to	DyCAST-BP3 retriever (t = 95)
retriever (t = 97)
noise by avoiding over-fragmentation of tokens. For voice	retriever (t = 99)3.81	3.89	97.8
3.87	4.23	97.7
3.94	4.61	97.2
conversion, DyCAST achieves strong speaker similarity and intelligibility, comparable to FocalCodec. Performance degrades gracefully as the frame rate is reduced, indicat- ing that the learned discrete units retain speaker-relevant information even at low temporal resolution.
Retrieval-Augmented Decoding. To simulate a realis- tic deployment scenario, we construct a large candidate pool containing 32-dimensional continuous latents extracted from the compressor. The pool includes all utterances from LibriSpeech train-clean-100, dev-clean, and test-clean, totaling approximately 20M vectors. This setting reflects a practical use case in which the re- trieval database contains diverse speakers, utterances, and acoustic conditions, while also naturally including material from the current speaker, as speakers tend to repeat similar acoustic patterns over time. Although a speaker-specific pool would likely yield stronger results, this deliberately challenging setup allows us to assess the robustness of the learned latent space and its ability to retrieve relevant acous- tic content from a large, heterogeneous database. For effi- ciency, the retrieval pool is indexed using an inverted file (IVF) index (Zobel & Moffat, 2006) implemented with the FAISS library (Johnson et al., 2019). We use nlist = 4096 inverted lists, probe nprobe = 16 lists at query time, and train the IVF index coarse clustering on a randomly subsampled set of 500k vectors.
Table 3 evaluates the impact of retrieval-augmented de- coding on LibriSpeech resynthesis across different simi- larity thresholds t . Overall, retrieval consistently improves


w/o retriever
DyCAST-BP5 retriever (t = 95)3.97
3.808.84
7.2296.5
97.2retriever (t = 97)3.918.0896.9retriever (t = 99)3.968.8796.5
intelligibility and speaker similarity at low token rates, while preserving naturalness when applied conservatively. For all DyCAST variants, moderate similarity thresholds (t ? [95, 97]) yield the most consistent gains. In this regime, retrieval reduces dWER relative to decoding without a re-
triever, with the largest improvements observed for lower- rate variants. This suggests that retrieval is particularly effective when token sequences are short and reconstruc- tion ambiguity is higher, allowing the decoder to recover fine-grained acoustic detail without increasing bitrate. Im- portantly, speaker similarity improves systematically with retrieval at moderate thresholds, indicating that retrieved exemplars reinforce speaker-specific characteristics rather than introducing identity drift. At the same time, UTMOS remains stable or decreases only marginally, indicating that retrieval does not harm perceived naturalness when con- strained by a sufficiently high similarity threshold. In con- trast, very high thresholds (t = 99) closely match the no- retrieval baseline across all metrics, effectively disabling the mechanism by being overly selective. We also note that UTMOS is a noisy metric and should be interpreted only as a coarse indicator of perceptual quality (Della Libera et al., 2025a). Consistent with this observation, informal listen- ing suggests that for UTMOS values above 3.50, similarity plays a more dominant role in perceived quality, with even small improvements corresponding to audible gains.

5.2 Discriminative Tasks
We evaluate the representational quality of DyCAST discrete tokens on automatic speech recognition (ASR), speaker identification (SI), and speech emotion recogni- tion (SER) probing tasks (Mousavi et al., 2024a). For ASR and SI, we use LibriSpeech, while for SER we use IEMO- CAP (Busso et al., 2008). All experiments follow the same shallow probing setup as in (Della Libera et al., 2025a). We report word error rate (WER) for ASR and error rate (ER) for SI and SER. Additional details are provided in Appendix D.3-D.5.
Results for discriminative probing tasks are reported in Ta- ble 4. Overall, DyCAST performs well despite operating at substantially lower frame rates than fixed-rate baselines. On ASR, DyCAST-CA achieves the best WER among all codecs, outperforming both fixed-frame-rate speech codecs and prior discrete tokenizers, while using significantly fewer tokens. This highlights the benefit of character-aligned tok- enization for preserving fine-grained linguistic information that is directly relevant for recognition. As the frame rate is reduced further, performance degrades gracefully. Vari- ants relying on predicted boundaries maintain competitive WER at moderate rates, while extremely low-frame-rate configurations exhibit higher WER, reflecting the expected trade-off between compression and phonetic resolution. Im- portantly, this degradation is consistent with trends observed in fixed-rate codecs at comparable or higher bitrates. For SI, DyCAST achieves error rates comparable to strong base- lines, indicating that speaker-related cues are well preserved even under aggressive temporal compression. Although fixed-frame-rate tokenizers optimized for speaker modeling (e.g. WavTokenizer) achieve lower SI error rates, DyCAST offers a more favorable balance between speaker informa- tion retention and token efficiency. On SER, performance across all codecs remains relatively close, suggesting that coarse prosodic and affective cues are robust to different tokenization strategies. DyCAST performs on par with fixed-rate baselines, indicating that variable-rate tokeniza- tion does not impair the extraction of emotional content, despite operating at lower frame rates. Overall, these results indicate that DyCAST tokens retain rich linguistic, speaker, and paralinguistic information even at substantially reduced rates. In particular, the strong ASR performance of the character-aligned variant suggests that explicitly modeling durations and linguistic alignment yields discrete tokens that are well suited for speech understanding tasks.
5.3 Text-To-Speech
We evaluate the generative capabilities of DyCAST on text- to-speech (TTS) using LibriSpeech, with character-level text as input and speaker embeddings for speaker condi- tioning. Performance is assessed using the same metrics as for speech resynthesis, namely UTMOS, dWER, and Sim. Following Della Libera et al. (2025a), we employ an

Table 4. Downstream modeling performance. Shallow prob- ing heads are used for discriminative tasks (ASR - LibriSpeech, SI - LibriSpeech, SER - IEMOCAP), transformer-based mod- els are used for generative tasks (TTS - LibriSpeech). Best and second-best results are highlighted.	

ASRSISERTTSCodec	Frame	Bitrate
Rate (Hz) ? (kbps) ?WER ?ER ?ER ?UTMOS ? dWER ? Sim ?
EnCodec75.01.5027.893.0047.001.7164.2883.2DAC50.01.0035.893.2745.901.3447.0685.9WavLM6-KM50.00.4519.0422.3042.903.7438.6788.7SpeechTokenizer50.01.0014.972.7341.502.6935.4689.2SemantiCodec25.00.6541.4215.9051.602.8248.3891.4Mimi12.50.6922.985.4344.703.1128.6393.6WavTokenizer40.00.4835.622.4449.803.6847.5692.8BigCodec80.01.0426.412.3447.503.4354.4389.4Stable Codec25.00.7016.8516.5046.543.1949.2888.8FocalCodec50.00.6517.634.4845.604.1128.1093.3DyCAST-CA	~14.4	~0.92	13.05  9.03 49.54	4.20†	3.42†  96.2†
DyCAST-BP1	~17.5	~1.12	19.98  7.34 51.61	3.91	11.57	93.2
DyCAST-BP3	~9.0	~0.57	27.80  7.42 50.00	4.00	14.56	92.6
DyCAST-BP5	~6.2	~0.40	40.27  11.50 50.23	4.02	15.85	92.8 
† Non-autoregressive one-to-one architecture, defined only for DyCAST-CA.
autoregressive architecture over speech tokens for TTS, re- flecting the formulation of TTS as text-conditioned speech language modeling. The only exception is the DyCAST- CA variant: thanks to the hard character-level alignment provided by the tokenizer, characters and speech tokens are in one-to-one correspondence. This enables the use of a non-autoregressive TTS model that directly maps each input character to its corresponding speech token. Details on the model architecture, hyperparameters, and training procedure are provided in Appendix D.6.
As shown in Table 4, DyCAST achieves strong performance across naturalness, intelligibility, and speaker similarity while operating at substantially reduced frame rates. Since the TTS model is autoregressive, lower frame rates directly translate into shorter sequences, yielding a more favorable learning regime by reducing exposure bias and easing long- range dependency modeling. Notably, DyCAST-CA clearly stands out, achieving by far the best TTS performance across all metrics. This result is enabled by the non-autoregressive one-to-one architecture uniquely supported by DyCAST- CA, which eliminates sequential token prediction altogether and is particularly effective in this limited-data regime. In addition, this architecture enables extremely fast inference, as generation is performed in a single parallel pass and does not require sampling or rescoring multiple hypotheses. Overall, these results highlight the dual benefit of DyCAST: efficient, controllable, low-rate tokenization that simplifies autoregressive generation, and character-aligned representa- tions that enable state-of-the-art quality and high inference efficiency in data-limited TTS scenarios.
6 Conclusions
We introduced DyCAST, a speech codec that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST pro- duces substantially shorter token sequences than fixed- frame-rate codecs while maintaining competitive speech resynthesis quality and downstream performance, resulting



in improved efficiency and more tractable transformer-based sequence modeling. A key advantage of DyCAST is its flex- ibility, enabling content-adaptive frame rates and explicit control over duration, bitrate, and reconstruction quality. Combined with the proposed retrieval-augmented decoding strategy for improved resynthesis, DyCAST offers a promis- ing foundation for next-generation speech tokenization.
References
Ahia, O., Kumar, S., Gonen, H., Hofmann, V., Limisiewicz, T., Tsvetkov, Y., and Smith, N. A. MAGNET: Improv- ing the multilingual fairness of language models with adaptive gradient-based tokenization. In International Conference on Neural Information Processing Systems (NeurIPS), volume 37, pp. 47790-47814, 2024.
Baas, M., van Niekerk, B., and Kamper, H. Voice conversion with just nearest neighbors. In Interspeech, pp. 2053- 2057, 2023.
Baevski, A., Zhou, Y., Mohamed, A., and Auli, M. wav2vec 2.0: A framework for self-supervised learning of speech representations. In International Conference on Neural Information Processing Systems (NeurIPS), pp. 12449- 12460, 2020.
Bai, H., Likhomanenko, T., Zhang, R., Gu, Z., Aldeneh, Z., and Jaitly, N. dMel: Speech tokenization made simple. arXiv preprint arXiv:2407.15835, 2024.
Borsos, Z., Marinier, R., Vincent, D., Kharitonov, E., Pietquin, O., Sharifi, M., Roblek, D., Teboul, O., Grang- ier, D., Tagliasacchi, M., and Zeghidour, N. Audi- oLM: A language modeling approach to audio generation. IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP), 31:2523-2533, 2023.
Busso, C., Bulut, M., Lee, C.-C., Kazemzadeh, A., Mower,
E., Kim, S., Chang, J. N., Lee, S., and Narayanan, S. S. IEMOCAP: Interactive emotional dyadic motion capture database. Language Resources and Evaluation, 42(4): 335-359, 2008.
Chang, H.-J., Gong, H., Wang, C., Glass, J., and Chung, Y.-A. DC-Spin: A speaker-invariant speech tokenizer for spoken language models. In Interspeech, pp. 5723-5727, 2025.
Chen, S., Wang, C., Chen, Z., Wu, Y., Liu, S., Chen, Z., Li,
J., Kanda, N., Yoshioka, T., Xiao, X., Wu, J., Zhou, L.,
Ren, S., Qian, Y., Qian, Y., Wu, J., Zeng, M., Yu, X., and Wei, F. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE Journal of Selected Topics in Signal Processing, pp. 1505-1518, 2022.
Chen, S., Wang, C., Wu, Y., Zhang, Z., Zhou, L., Liu, S.,
Chen, Z., Liu, Y., Wang, H., Li, J., He, L., Zhao, S.,

and Wei, F. Neural codec language models are zero-shot text to speech synthesizers. IEEE Transactions on Audio, Speech and Language Processing (TASLP), 33:705-718, 2025.
Comanici, G., Bieber, E., Schaekermann, M., Pasupat, I., Sachdeva, N., Dhillon, I., Blistein, M., Ram, O., Zhang, D., Rosen, E., et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025.
Copet, J., Kreuk, F., Gat, I., Remez, T., Kant, D., Synnaeve, G., Adi, Y., and Defossez, A. Simple and controllable music generation. In International Conference on Neural Information Processing Systems (NeurIPS), volume 36,
pp. 47704-47720, 2023.
Cosentino, J., Pariente, M., Cornell, S., Deleforge, A., and Vincent, E. LibriMix: An open-source dataset for generalizable speech separation. arXiv preprint arXiv:2005.11262, 2020.
Cuervo, S., Lan´cucki, A., Marxer, R., Rychlikowski, P., and Chorowski, J. Variable-rate hierarchical CPC leads to acoustic unit discovery in speech. In International Conference on Neural Information Processing Systems (NeurIPS), volume 35, pp. 34995-35006, 2022.
DeepSeek-AI, Liu, A., Feng, B., Xue, B., Wang, B., Wu, B.,
Lu, C., Zhao, C., Deng, C., Zhang, C., et al. DeepSeek- V3 technical report. arXiv preprint arXiv:2412.19437, 2025.
De´fossez, A., Copet, J., Synnaeve, G., and Adi, Y. High fidelity neural audio compression. Transactions on Ma- chine Learning Research (TMLR), 2023.
Della Libera, L., Subakan, C., and Ravanelli, M. Focal mod- ulation networks for interpretable sound classification. In IEEE International Conference on Acoustics, Speech, and Signal Processing Workshops (ICASSPW), pp. 853-857, 2024.
Della Libera, L., Paissan, F., Subakan, C., and Ravanelli, M. FocalCodec: Low-bitrate speech coding via focal modu- lation networks. In International Conference on Neural Information Processing Systems (NeurIPS), 2025a.
Della Libera, L., Subakan, C., and Ravanelli, M. FocalCodec-Stream: Streaming low-bitrate speech coding via causal distillation. arXiv preprint arXiv:2509.16195, 2025b.
Dieleman, S., Nash, C., Engel, J., and Simonyan, K. Variable-rate discrete representation learning. arXiv preprint arXiv:2103.06089, 2021.



De´fossez, A., Mazare´, L., Orsini, M., Royer, A., Pe´rez, P., Je´gou, H., Grave, E., and Zeghidour, N. Moshi: A speech-text foundation model for real-time dialogue. arXiv preprint arXiv:2410.00037, 2024.
Gong, Y., Jin, L., Deng, R., Zhang, D., Zhang, X., Cheng, Q., Fei, Z., Li, S., and Qiu, X. XY-Tokenizer: Mitigating the semantic-acoustic conflict in low-bitrate speech codecs. arXiv preprint arXiv:2506.23325, 2025.
Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., Yang, A., Fan, A., et al. The Llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024.
Graves, A., Ferna´ndez, S., Gomez, F., and Schmidhuber,
J. Connectionist temporal classification: labelling unseg- mented sequence data with recurrent neural networks. In International Conference on Machine Learning (ICML),
pp. 369-376, 2006.
Guo, Y., Li, Z., Wang, H., Li, B., Shao, C., Zhang, H., Du, C., Chen, X., Liu, S., and Yu, K. Recent advances in discrete speech tokens: A review. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2025.
Har-Tuv, N., Tal, O., and Adi, Y. PAST: Phonetic-acoustic speech tokenizer. In Interspeech, pp. 3509-3513, 2025.
Hsu, M.-H., Tseng, L.-H., yi Lee, H., and Wu, Z. TASLA: Text-aligned speech tokens with multiple layer- aggregation. arXiv preprint arXiv:2510.14934, 2025.
Hsu, W.-N., Bolte, B., Tsai, Y.-H. H., Lakhotia, K., Salakhutdinov, R., and Mohamed, A. HuBERT: Self- supervised speech representation learning by masked pre- diction of hidden units. IEEE/ACM Transactions on Au- dio, Speech, and Language Processing, 29:3451-3460, 2021.
Hwang, S., Wang, B., and Gu, A. Dynamic chunking for end-to-end hierarchical sequence modeling. arXiv preprint arXiv:22507.07955, 2025.
Ji, S., Jiang, Z., Wang, W., Chen, Y., Fang, M., Zuo, J.,
Yang, Q., Cheng, X., Wang, Z., Li, R., Zhang, Z., Yang,
X., Huang, R., Jiang, Y., Chen, Q., Zheng, S., Wang, W., and Zhao, Z. WavTokenizer: An efficient acoustic discrete codec tokenizer for audio language modeling. In International Conference on Learning Representations (ICLR), 2025.
Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., and others, E. B. H. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024a.

Jiang, X., Peng, X., Zhang, Y., and Lu, Y. Universal speech token learning via low-bitrate neural codec and pretrained representations. IEEE Journal of Selected Topics in Sig- nal Processing, pp. 1-13, 2024b.
Johnson, J., Douze, M., and Je´gou, H. Billion-scale similar- ity search with GPUs. IEEE Transactions on Big Data, 7 (3):535-547, 2019.
Ju, Z., Wang, Y., Shen, K., Tan, X., Xin, D., Yang, D., Liu,
Y., Leng, Y., Song, K., Tang, S., Wu, Z., Qin, T., Li, X.-
Y., Ye, W., Zhang, S., Bian, J., He, L., Li, J., and Zhao,
S. NaturalSpeech 3: Zero-shot speech synthesis with factorized codec and diffusion models. In International Conference on Machine Learning (ICML), 2024.
Kong, J., Kim, J., and Bae, J. HiFi-GAN: generative ad- versarial networks for efficient and high fidelity speech synthesis. In International Conference on Neural Infor- mation Processing Systems (NeurIPS), 2020.
Kumar, R., Seetharaman, P., Luebs, A., Kumar, I., and Ku- mar, K. High-fidelity audio compression with improved RVQGAN. In International Conference on Neural Infor- mation Processing Systems (NeurIPS), 2023.
Labiausse, T., Mazare´, L., Grave, E., De´fossez, A., and Zeghidour, N. High-fidelity simultaneous speech-to- speech translation. In International Conference on Ma- chine Learning (ICML), 2025.
Li, J., Lin, X., Li, Z., Huang, S., Wang, Y., Wang, C., Zhan, Z., and Wu, Z. DualCodec: A low-frame-rate, semantically-enhanced neural audio codec for speech generation. In Interspeech 2025, pp. 4883-4887, 2025a.
Li, J., Qian, Y., Hu, Y., Zhang, L., Wang, X., Lu, H., Thakker, M., Li, J., Zhao, S., and Wu, Z. FlexiCodec: A dynamic neural audio codec for low frame rates. arXiv preprint arXiv:2510.00981, 2025b.
Liu, H., Xu, X., Yuan, Y., Wu, M., Wang, W., and Plumbley,
M. D. SemantiCodec: An ultra low bitrate semantic audio codec for general sound. IEEE Journal of Selected Topics in Signal Processing, 18(8):1448-1461, 2024.
Loshchilov, I. and Hutter, F. Decoupled weight decay reg- ularization. In International Conference on Learning Representations (ICLR), 2019.
Lu, Y.-J., Gaur, Y., Zhou, W., Muller, B., Villalba, J., Dehak, N., Zettlemoyer, L., Ghosh, G., Lewis, M., Iyer, S., and Le, D. Latent speech-text transformer. arXiv preprint arXiv:2510.06195, 2025.
Mentzer, F., Minnen, D., Agustsson, E., and Tschannen,
M. Finite scalar quantization: VQ-VAE made simple. In International Conference on Learning Representations (ICLR), 2024.



Messica, S. and Adi, Y. NAST: Noise aware speech tok- enization for speech language models. In Interspeech, pp. 4169-4173, 2024.
Mousavi, P., Della Libera, L., Duret, J., Ploujnikov, A., Subakan, C., and Ravanelli, M. DASB - discrete audio and speech benchmark. arXiv preprint arXiv:2406.14294, 2024a.
Mousavi, P., Duret, J., Zaiem, S., Della Libera, L., Plou- jnikov, A., Subakan, C., and Ravanelli, M. How should we extract discrete audio tokens from self-supervised models? In Interspeech, pp. 2554-2558, 2024b.
Mousavi, P., Maimon, G., Moumen, A., Petermann, D., Shi, J., Wu, H., Yang, H., Kuznetsova, A., Ploujnikov, A., Marxer, R., Ramabhadran, B., Elizalde, B., Lugosch, L., Li, J., Subakan, C., Woodland, P., Kim, M., Lee, H.-y., Watanabe, S., Adi, Y., and Ravanelli, M. Discrete audio tokens: More than a survey! Transactions on Machine Learning Research (TMLR), 2025.
Nawrot, P., Chorowski, J., Lancucki, A., and Ponti, E. M. Efficient transformers with dynamic token pooling. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 6403-6417, 2023.
Nguyen, T. A., Muller, B., Yu, B., Costa-jussa, M. R., El-
bayad, M., Popuri, S., Ropers, C., Duquenne, P.-A., Al- gayres, R., Mavlyutov, R., Gat, I., Williamson, M., Syn- naeve, G., Pino, J., Sagot, B., and Dupoux, E. SpiRit-LM: Interleaved spoken and written language model. Trans- actions of the Association for Computational Linguistics (TACL), 13:30-52, 2025.
Pagnoni, A., Pasunuru, R., Rodriguez, P., Nguyen, J., Muller, B., Li, M., Zhou, C., Yu, L., Weston, J. E., Zettle- moyer, L., Ghosh, G., Lewis, M., Holtzman, A., and Iyer, S. Byte latent transformer: Patches scale better than tokens. In Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 9238-9258, 2025.
Panayotov, V., Chen, G., Povey, D., and Khudanpur, S. Lib- riSpeech: An ASR corpus based on public domain audio books. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5206-5210, 2015.
Parker, J. D., Smirnov, A., Pons, J., Carr, C., Zukowski, Z., Evans, Z., and Liu, X. Scaling transformers for low- bitrate high-quality speech coding. In International Con- ference on Learning Representations (ICLR), 2025.
Pratap, V., Xu, Q., Sriram, A., Synnaeve, G., and Collobert,
R. MLS: A large-scale multilingual dataset for speech research. In Interspeech, pp. 2757-2761, 2020.

Pratap, V., Tjandra, A., Shi, B., Tomasello, P., Babu, A., Kundu, S., Elkahky, A., Ni, Z., Vyas, A., Fazel-Zarandi, M., Baevski, A., Adi, Y., Zhang, X., Hsu, W.-N., Con- neau, A., and Auli, M. Scaling speech technology to 1,000+ languages. Journal of Machine Learning Research (JMLR), 25, 2024.
Radford, A., Kim, J. W., Xu, T., Brockman, G., Mcleavey, C., and Sutskever, I. Robust speech recognition via large- scale weak supervision. In International Conference on Machine Learning (ICML), volume 202, pp. 28492- 28518, 2023.
Ravanelli, M., Parcollet, T., Plantinga, P., Rouhe, A., Cor- nell, S., Lugosch, L., Subakan, C., Dawalatabad, N., Heba, A., Zhong, J., Chou, J.-C., Yeh, S.-L., Fu, S.-
W., Liao, C.-F., Rastorgueva, E., Grondin, F., Aris, W.,
Na, H., Gao, Y., Mori, R. D., and Bengio, Y. Speech- Brain: A general-purpose speech toolkit. arXiv preprint arXiv:2106.04624, 2021.
Ravanelli, M., Parcollet, T., Moumen, A., de Langen, S., Subakan, C., Plantinga, P., Wang, Y., Mousavi, P., Della Libera, L., Ploujnikov, A., Paissan, F., Borra, D., Zaiem, S., Zhao, Z., Zhang, S., Karakasidis, G., Yeh, S.-L., Champion, P., Rouhe, A., Braun, R., Mai, F., Zuluaga- Gomez, J., Mousavi, S. M., Nautsch, A., Nguyen, H., Liu, X., Sagar, S., Duret, J., Mdhaffar, S., Laperrie`re, G., Rouvier, M., Mori, R. D., and Este`ve, Y. Open-source conversational AI with SpeechBrain 1.0. Journal of Ma- chine Learning Research (JMLR), 25(333):1-11, 2024.
Reddy, C. K., Gopal, V., and Cutler, R. DNSMOS P.835: A non-intrusive perceptual objective speech quality metric to evaluate noise suppressors. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.
Ren, K., Qin, J., Zheng, L., Yang, Z., Zhang, W., Qiu, L., and Yu, Y. Deep recurrent survival analysis. In AAAI Conference on Artificial Intelligence, 2019.
Rix, A., Beerends, J., Hollier, M., and Hekstra, A. Percep- tual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 749-752, 2001.
Saeki, T., Xin, D., Nakata, W., Koriyama, T., Takamichi, S., and Saruwatari, H. UTMOS: UTokyo-SaruLab system for VoiceMOS challenge 2022. In Interspeech, pp. 4521- 4525, 2022.
Singer, J. D. and Willett, J. B. It's about time: Using discrete- time survival analysis to study duration and the timing of events. Journal of Educational Statistics, 18(2):155-195, 1993.



Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. OpenAI GPT-5 system card. arXiv preprint arXiv:2601.03267, 2025.
Siuzdak, H. Vocos: Closing the gap between time-domain and fourier-based neural vocoders for high-quality au- dio synthesis. In International Conference on Learning Representations (ICLR), 2024.
Slagle, K. SpaceByte: Towards deleting tokenization from large language modeling. In International Conference on Neural Information Processing Systems (NeurIPS), volume 37, pp. 124925-124950, 2024.
Song, Y., Chen, J., Zhuang, X., Du, C., Ma, Z., Wu, J.,
Cong, J., Jia, D., Chen, Z., Wang, Y., Wang, Y., and Chen,
X. MagiCodec: Simple masked gaussian-injected codec for high-fidelity reconstruction and generation. arXiv preprint arXiv:2506.00385, 2025.
Taal, C. H., Hendriks, R. C., Heusdens, R., and Jensen, J. An algorithm for intelligibility prediction of time-frequency weighted noisy speech. IEEE Transactions on Audio, Speech and Language Processing (TASLP), pp. 2125- 2136, 2011.
Tan, W., Inaguma, H., Dong, N., D. Tomasello, P., and Ma, X. SSR: Alignment-aware modality connector for speech language models. In Salesky, E., Federico, M., and Anastasopoulos, A. (eds.), International Conference on Spoken Language Translation (IWSLT 2025), pp. 56- 75, 2025.
Tian, J., Shi, J., Chen, W., Arora, S., Masuyama, Y.,
Maekaku, T., Wu, Y., Peng, J., Bharadwaj, S., Zhao, Y.,
Cornell, S., Peng, Y., Yue, X., Yang, C.-H. H., Neubig, G., and Watanabe, S. ESPnet-SpeechLM: An open speech language model toolkit. In Conference of the Nations of the Americas Chapter of the Association for Computa- tional Linguistics (NAACL): Human Language Technolo- gies (System Demonstrations), pp. 116-124, 2025.
Tseng, L.-H., Chen, Y.-C., Lee, K.-Y., Shiu, D.-S., and yi Lee, H. TASTE: Text-aligned speech tokenization and embedding for spoken language modeling. arXiv preprint arXiv:2504.07053, 2025.
Valentini-Botinhao, C., Wang, X., Takaki, S., and Yamag- ishi, J. Investigating RNN-based speech enhancement methods for noise-robust text-to-speech. In Speech Syn- thesis Workshop, pp. 146-152, 2016.
Van Kuyk, S., Kleijn, W. B., and Hendriks, R. C. On the information rate of speech communication. In IEEE In- ternational Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5625-5629, 2017.

Videau, M., Idrissi, B. Y., Leite, A., Schoenauer, M., Tey- taud, O., and Lopez-Paz, D. From bytes to ideas: Lan- guage modeling with autoregressive U-Nets. In Inter- national Conference on Neural Information Processing Systems (NeurIPS), 2025.
Wang, H., Guo, Y., Shao, C., Li, B., Chen, X., and Yu, K. CodecSlime: Temporal redundancy compression of neu- ral speech codec via dynamic frame rate. arXiv preprint arXiv:2506.21074, 2025a.
Wang, Y., Chen, D., Zhang, X., Zhang, J., Li, J., and Wu,
Z. TaDiCodec: Text-aware diffusion speech tokenizer for speech language modeling. In International Conference on Neural Information Processing Systems (NeurIPS), 2025b.
Wang, Z., Zhu, X., Zhang, Z., Lv, Y., Jiang, N., Zhao, G., and Xie, L. SELM: Speech enhancement using dis- crete tokens and language models. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 11561-11565, 2024.
Wang, Z.-Q. et al. Sequential multi-frame neural beam- forming for speech separation and enhancement. In IEEE Spoken Language Technology Workshop (SLT), pp. 905- 911, 2021.
Wichern, G., Antognini, J., Flynn, M., Zhu, L. R., McQuinn, E., Crow, D., Manilow, E., and Roux, J. L. WHAM!: Extending speech separation to noisy environments. In Interspeech, pp. 1368-1372, 2019.
Wu, H., Kanda, N., Emre Eskimez, S., and Li, J. TS3-Codec: Transformer-based simple streaming single codec. In Interspeech, pp. 604-608, 2025.
Xin, D., Tan, X., Takamichi, S., and Saruwatari, H. Big- Codec: Pushing the limits of low-bitrate neural speech codec. arXiv preprint arXiv:2409.05377, 2024.
Yamagishi, J., Veaux, C., and MacDonald, K. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR), 6:15, 2017.
Yang, D., Liu, S., Guo, H., Zhao, J., Wang, Y., Wang, H.,
Ju, Z., Liu, X., Chen, X., Tan, X., Wu, X., and Meng,
H. M. ALMTokenizer: A low-bitrate and semantic-rich audio codec tokenizer for audio language modeling. In International Conference on Machine Learning (ICML), 2025.
Yang, J., Li, C., Dai, X., and Gao, J. Focal modulation networks. In International Conference on Neural Infor- mation Processing Systems (NeurIPS), 2022.


Ye, Z., Zhu, X., Chan, C.-M., Wang, X., Tan, X., Lei, J.,
Peng, Y., Liu, H., Jin, Y., Dai, Z., Lin, H., Chen, J., Du,
X., Xue, L., Chen, Y., Li, Z., Xie, L., Kong, Q., Guo, Y., and Xue, W. Llasa: Scaling train-time and inference- time compute for Llama-based speech synthesis. arXiv preprint arXiv:2502.04128, 2025.
Zeghidour, N., Luebs, A., Omran, A., Skoglund, J., and Tagliasacchi, M. SoundStream: An end-to-end neural audio codec. IEEE/ACM Transactions on Audio, Speech, and Language Processing, pp. 495-507, 2021.
Zeghidour, N., Kharitonov, E., Orsini, M., Volhejn, V., de Marmiesse, G., Grave, E., Pe´rez, P., Mazare´, L., and De´fossez, A. Streaming sequence-to-sequence learn- ing with delayed streams modeling. arXiv preprint arXiv:2509.08753, 2025.
Zen, H., Tokuda, K., and Black, A. W. Statistical parametric speech synthesis. Speech Communication, 51(11):1039- 1064, 2009.
Zen, H., Dang, V., Clark, R., Zhang, Y., Weiss, R. J., Jia, Y., Chen, Z., and Wu, Y. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Interspeech, 2019.
Zhang, H., Guo, Y., Li, Z., Hao, X., Chen, X., and Yu, K. Unlocking temporal flexibility: Neural speech codec with variable frame rate. In Interspeech, pp. 5003-5007, 2025.
Zhang, X., Zhang, D., Li, S., Zhou, Y., and Qiu, X. Speech- Tokenizer: Unified speech tokenizer for speech large lan- guage models. In International Conference on Learning Representations (ICLR), 2024.
Zhao, Y., Xiong, Y., and Kra¨henbu¨ hl, P. Image and video tokenization with binary spherical quantization. In Inter- national Conference on Learning Representations (ICLR), 2025.
Zheng, R.-C., Liu, W., Du, H.-P., Zhang, Q., Deng, C., Chen, Q., Wang, W., Ai, Y., and Ling, Z.-H. Say more with less: Variable-frame-rate speech tokenization via adaptive clustering and implicit duration coding. In AAAI Conference on Artificial Intelligence, 2026.
Zheng, Y., Tu, W., Kang, Y., Chen, J., Zhang, Y., Xiao, L., Yang, Y., and Ma, L. FreeCodec: A disentangled neural speech codec with fewer tokens. In Interspeech,
pp. 4878-4882, 2025.
Zobel, J. and Moffat, A. Inverted files for text search engines.
ACM Computing Surveys (CSUR), 38(2):6-es, 2006.

A Datasets
The following datasets were used in this work:
• LibriSpeech (Panayotov et al., 2015) is a large-scale corpus of English read speech derived from audiobooks in the LibriVox project. It contains approximately 1000 hours of speech sampled at 16 kHz, with predefined training, validation, and test splits. License: CC BY 4.0.
• LibriTTS (Zen et al., 2019) is a corpus designed for text-to-speech research, constructed from the same source as LibriSpeech. It consists of 585 hours of transcribed speech with predefined training, validation, and test splits. License: CC BY 4.0.
• MLS (Pratap et al., 2020) is an extension of LibriSpeech to multiple languages, including English, German, Dutch, French, Spanish, Italian, Portuguese and Polish. It provides approximately 44,500 hours of transcribed English speech and about 6000 hours from other languages. License: CC BY 4.0.
• VoiceBank (Valentini-Botinhao et al., 2016) is a dataset primarily used for speech enhancement, including 11,572 utterances from 28 speakers in the training set (noise at 0 dB, 5 dB, 10 dB, and 15 dB), and 872 utterances from 2 unseen speakers in the test set (noise at 2.5 dB, 7.5 dB, 12.5 dB, and 17.5 dB). License: CC BY 4.0.
• LibriMix (Cosentino et al., 2020) is a dataset for speech separation and enhancement, created by mixing LibriSpeech utterances with noise from the WHAM! (Wichern et al., 2019) corpus. It provides mixtures of two or three speakers at different signal-to-noise ratios. License: MIT.
• VCTK (Yamagishi et al., 2017) is a corpus of English speech recordings from 110 speakers with various accents. It is widely used for speaker adaptation, text-to-speech, and voice conversion tasks. License: CC BY 4.0.
• IEMOCAP (Busso et al., 2008) is a dataset designed for emotion recognition, consisting of scripted and improvised dialogues performed by 10 actors. It includes audio, video, and textual transcriptions with emotion labels such as happiness, sadness, and anger. License: https://sail.usc.edu/iemocap/iemocap release.htm.
B Baselines
We compare DyCAST against a diverse set of fixed-frame-rate neural audio codecs covering general audio, speech-focused, single- and multi-codebook, acoustic, semantic, and hybrid (additional details are provided in Table 5):
• EnCodec (De´fossez et al., 2023): A general-purpose neural audio codec supporting both causal and non-causal operation, trained on large-scale heterogeneous audio data and widely used as a strong baseline for speech and audio compression.
• DAC (Kumar et al., 2023): A high-quality non-causal audio codec optimized for perceptual reconstruction, trained on diverse speech and music datasets and commonly used in downstream speech tasks.
• WavLM6-KM (Wang et al., 2024): A speech codec obtained by clsutering WavLM representations from the 6-th transfomer layer.
• SpeechTokenizer (Zhang et al., 2024): A multi-codebook speech tokenizer designed to disentangle content and speaker information across codebooks, primarily trained on LibriSpeech.
• SemantiCodec (Liu et al., 2024): A general audio codec trained on large-scale multimodal datasets, explicitly targeting semantic preservation across speech and non-speech audio.
• Mimi (De´fossez et al., 2024): A large-scale causal speech tokenizer trained on millions of hours of speech, designed for streaming and speech language modeling applications.
• WavTokenizer (Ji et al., 2025): A unified audio tokenizer trained on mixed speech and music data, supporting multilingual and multi-domain speech modeling.
• BigCodec (Xin et al., 2024): A speech-focused codec trained on LibriSpeech, emphasizing efficient discrete representa- tions for downstream speech generation and modeling tasks.
• Stable Codec (Parker et al., 2025): A transformer-based neural speech codec trained on large-scale speech corpora, supporting optional causal inference.
• FocalCodec (Della Libera et al., 2025a): A low-bitrate speech codec based on focal modulation and binary spherical quantization, serving as the closest architectural baseline to DyCAST.


Table 5. Baseline codecs.
Codec			Training Datasets	Hours Multilingual Audio Domain			Checkpoint	License EnCodec (De´fossez et al., 2023)		DNS, CommonVoice, AudioSet, FSD50K, Jamendo		17k+	Yes	General		encodec 24khz		MIT DAC (Kumar et al., 2023)	DAPS, DNS, CommonVoice, VCTK, MUSDB, Jamendo		10k+	Yes	General	weights 16khz.pth		MIT

WavLM6-KM (Wang et al., 2024)	Subset of LibriSpeech (in addition to Libri-Light,

460

No	Speech	discrete-wavlm-codec	Apache 2.0

GigaSpeech, and VoxPopuli English for WavLM pretraining) (+ 94k)

SpeechTokenizer (Zhang et al., 2024)	LibriSpeech	960	No	Speech	speechtokenizer hubert avg	Apache 2.0

GigaSpeech, subset of OpenSLR, Million Song Dataset, MedleyDB, MUSDB18, AudioSet, WavCaps, VGGSound


20k+	Yes	General	semanticodec tokenrate 50	MIT

Mimi (De´fossez et al., 2024)	Predominantly English speech (in addition to Libri-Light,	7M


Likely	Speech	mimi	CC BY 4.0

GigaSpeech, and VoxPopuli English for WavLM pretraining) (+ 94k)





GigaSpeech, and VoxPopuli English for WavLM pretrain

C Hyperparameters and Training Details
We summarize the main hyperparameters and training settings used for all DyCAST components. All learnable modules are trained on LibriTTS (Zen et al., 2019) (585 hours from 2456 speakers) using full utterances and identical optimizer settings, except for the decoder, which requires a different training setup. For all other modules, training is performed using the AdamW (Loshchilov & Hutter, 2019) optimizer with an initial learning rate of 0.0005, ß1 = 0.8, ß2 = 0.99, and a weight decay of 0.01. The learning rate is reduced by a factor of 0.9 when the validation loss does not improve by at least 0.0025. Gradients are clipped to a maximum L2 norm of 5, and training stops when the validation loss fails to decrease for several consecutive epochs.
Character Aligner. As a character aligner, we use MMS (Pratap et al., 2024), a 1B-parameter multilingual wav2vec2.0 ASR model pretrained on approximately 500k hours of speech across more than 1,000 languages and equipped with a CTC-based character prediction head. The model is kept frozen throughout training and is used to obtain character-level durations via simple argmax decoding. While more sophisticated alignment strategies such as Viterbi decoding (Graves et al., 2006) are possible, we adopt argmax decoding for its simplicity and efficiency.
Encoder. For the encoder, we use the pretrained WavLM-large2 model (Chen et al., 2022). In particular, we extract representations from the 6th transformer layer, which provide semantically rich features while retaining fine-grained acoustic information (Della Libera et al., 2025a; Baas et al., 2023). The encoder is kept frozen throughout training.
Compressor. The compressor takes as input 1024-dimensional WavLM features and processes them through 3 focal downscaling blocks (Della Libera et al., 2025a) while preserving the original temporal resolution (50 Hz), each with a hidden dimension of 1024. Each block uses 2 focal levels, a window size of 14, a focal factor of 4, and a layer scale initialization of 0.0001. A final projection maps the 1024-dimensional hidden states to 32-dimensional latent representations.
Chunker. Within the chunker, the boundary predictor adopts the same architecture as the compressor. A final binary classification head maps the 1024-dimensional hidden states to boundary logits. The dynamic downsampling operation itself is implemented as a simple frame-selection and does not introduce additional trainable parameters.
Quantizer. The scalar spherical quantizer discretizes the continuous 32-dimensional latent representations using 4 levels per dimension, resulting in an implicit codebook of size |C| = 432. In practice, it is infeasible to enumerate all codes explicitly; instead, we represent tokens by returning per-dimension indices in {0, 1, 2, 3}. The quantizer has no trainable parameters, and we set the entropy loss weight to 0.1.
Dechunker. Within the dechunker, the duration predictor follows the same architectural design but processes 32- dimensional pooled quantized latents. The model predicts a free mean parameter (a scalar value) together with a shared, learnable scalar dispersion parameter a. Dynamic upsampling is implemented by frame repetition and introduces no additional parameters; the length regularization strength is set to ? = 0.05.

2https://github.com/microsoft/unilm/tree/master/wavlm


Decompressor. The decompressor mirrors the compressor architecture, replacing focal downscaling blocks with focal upscaling blocks to reconstruct 1024-dimensional continuous representations from the quantized latent codes.

Decoder. The Vocos (Siuzdak, 2024) decoder operates on 1024-dimensional WavLM features and processes them through 8 ConvNeXt blocks with a hidden dimension of 512, a feed-forward dimension of 1536, a kernel size of 7, and padding of 3. For the STFT, we use an FFT size of 1024 samples and a hop length of 320. The feature-matching loss is computed using 80-dimensional log-Mel spectrograms with the same STFT configuration. The discriminator follows the convolutional architecture introduced in (Kong et al., 2020). Training is conducted on LibriTTS train-clean-100 using audio chunks of 7040 samples and a batch size of 16. We use the AdamW optimizer with an initial learning rate of 0.0002, ß1 of 0.8, ß2 of 0.99, and a weight decay of 0.01. The learning rate follows an exponential decay schedule with a factor of 0.999. Training continues until perceived audio quality saturates, which occurs after approximately 3M steps.
D Downstream Evaluation
In this section, we outline the downstream tasks used to evaluate the proposed codec and describe the corresponding experimental setups. Unless otherwise stated, all downstream models are trained using the AdamW optimizer (Loshchilov & Hutter, 2019) with a batch size of 16, an initial learning rate of 0.0001, ß1 = 0.8, ß2 = 0.99, weight decay of 0.01, and dropout of 0.1. The learning rate is reduced by a factor of 0.9 if validation loss does not improve within a margin of 0.0025. Gradients are clipped to a maximum L2 norm of 0.01. Training stops if validation loss does not decrease for several consecutive epochs. Each model is trained on a single NVIDIA H100 GPU with 80 GB of memory. Software for the downstream evaluation was implemented in Python using the SpeechBrain (Ravanelli et al., 2021; 2024) toolkit.
D.1 Speech Resynthesis (SR)
This task evaluates the ability of the codec to reconstruct high-quality speech from discrete tokens while preserving naturalness, speaker identity, and intelligibility. For English, we use the LibriSpeech (Panayotov et al., 2015) test-clean split. For multilingual evaluation, we use the dataset constructed by Della Libera et al. (2025a), which was built by selecting 100 utterances from each of the seven non-English languages in MLS (Pratap et al., 2020) (Dutch, French, German, Italian, Polish, Portuguese, and Spanish) for a total of 700 utterances3. To assess robustness to environmental noise, we additionally evaluate on the test sets of VoiceBank (Valentini-Botinhao et al., 2016) and Libri1Mix, constructed by mixing clean utterances from the first speaker of LibriMix (Cosentino et al., 2020) with noise from WHAM! (Wichern et al., 2019). Naturalness is measured using UTMOS (Saeki et al., 2022) for clean speech and DNSMOS (Reddy et al., 2022) for noisy speech. Intelligibility is evaluated using differential word error rate (dWER) (Wang et al., 2021), computed from transcriptions obtained with Whisper-small (Radford et al., 2023)4. Speaker fidelity is assessed via cosine similarity (Sim) between speaker embeddings extracted using WavLM-base-SV (Chen et al., 2022)5. We do not report signal-level metrics such as SNR, PESQ (Rix et al., 2001), or STOI (Taal et al., 2011), as they correlate poorly with perceived reconstruction quality (Parker et al., 2025; Wang et al., 2024; Della Libera et al., 2025a), and we avoid stronger ASR models to prevent masking reconstruction artifacts.
D.2 Voice Conversion (VC)
This task evaluates the ability of the codec to disentangle speaker identity from linguistic content despite its single-codebook design. Voice conversion is performed by converting speech from a source speaker to a target speaker using reference speech from the target speaker. For single-codebook baselines, including DyCAST, we follow the k-nearest neighbors (kNN) approach of Baas et al. (2023). Specifically, each frame in the reconstructed feature sequence (immediately before the decoder) is replaced by the average of the k = 4 nearest neighbors (measured by cosine similarity) from continuous features extracted from the target speaker reference utterance. For multi-codebook baselines, we follow the procedure of Zhang et al. (2024). Both source and reference speech are tokenized, and voice conversion is performed by concatenating the first codebook tokens from the source with the second-to-last codebook tokens from the reference, before decoding. If sequence lengths differ, the reference sequence is truncated or circularly padded as needed. Effective disentanglement of content and speaker information across codebooks is expected to yield strong voice conversion performance. We conduct voice conversion experiments on VCTK (Yamagishi et al., 2017), which contains parallel utterances from multiple speakers. For each sample, we randomly select (i) an utterance from a source speaker, (ii) the corresponding parallel utterance from a target speaker, and (iii) a different utterance from the same target speaker to serve as the reference. Among candidate

3https://zenodo.org/records/14791114
4https://huggingface.co/openai/whisper-small
5https://huggingface.co/microsoft/wavlm-base-sv


reference utterances, we select the longest to minimize padding. Repeating this procedure for each speaker and each of the approximately 24 parallel utterances yields a total of 2,521 evaluation samples. Performance is evaluated using the same metrics as in speech resynthesis: UTMOS, dWER, and Sim.
D.3 Automatic Speech Recognition (ASR)
This task evaluates the quality of the learned discrete representations through a probing setup, where a shallow model is trained on top of fixed codec tokens to assess their linguistic content. We use LibriSpeech (Panayotov et al., 2015) train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for eval- uation. The ASR probe consists of a 2-layer bidirectional LSTM with 512-dimensional hidden states, followed by a CTC (Graves et al., 2006) prediction head. The model is trained to predict either characters or byte-pair encoding (BPE) units, using BPE vocabularies of size 1000, except for Mimi, which achieves its best performance with a vocabulary size of 500. When a codec employs multiple codebooks, we compute a learned weighted sum of the embeddings from each codebook, following (Chen et al., 2022). The embedding layer is initialized using the discrete embeddings from the codec quantizer. Performance is reported in terms of word error rate (WER).
D.4 Speaker Identification (SI)
This task probes whether the learned discrete representations encode speaker identity through a probing setup, where a shallow model is trained on top of fixed codec tokens to assess their acoustic content. We use LibriSpeech (Panayotov et al., 2015), grouping utterances from train-clean-100 and train-clean-360 by speaker ID. The data are randomly split into training, validation, and test sets with a ratio of 80% / 10% / 10%. The SI probe closely mirrors the ASR setup. A 2-layer bidirectional LSTM with 512-dimensional hidden states processes the token sequence, and the output is aggregated using statistics pooling, followed by a cross-entropy classification head. When multiple codebooks are present, embeddings are combined via a learned weighted sum, as in ASR. Performance is reported in terms of speaker error rate (ER).
D.5 Speech Emotion Recognition (SER)
This task probes whether the learned representations capture paralinguistic information related to emotion. We again adopt a probing setup with a shallow classifier trained on top of frozen codec tokens. We use the IEMOCAP dataset (Busso et al., 2008), focusing on four emotion classes: sadness, happiness, anger, and neutral. Sessions 1-4 are used for training, session 5F for validation, and session 5M for testing. The SER setup is identical to SI, with the only difference being the number of output classes in the classification head. Performance is reported in terms of emotion error rate (ER).
D.6 Text-To-Speech (TTS)
This task evaluates the suitability of discrete speech representations for speech generation from text. We use LibriSpeech train-clean-100 and train-clean-360 for training, dev-clean for validation, and test-clean for evalua- tion. The test-clean split contains a small fraction of long utterances (approximately 4%) exceeding 20 seconds, which are largely absent from the training data; to reduce this train-test mismatch, we remove these long utterances from the test set. The model input consists of character-level text tokens, while the target sequence comprises speech tokens extracted from the corresponding utterances. We consider two TTS model variants. An autoregressive model is used for all codecs except DyCAST-CA, while a non-autoregressive model is used exclusively for DyCAST-CA to exploit the availability of character-aligned tokens. Performance is evaluated using UTMOS, dWER, and Sim.
Autoregressive TTS. The autoregressive model is a Llama 3 decoder (Grattafiori et al., 2024) with 12 layers, 4 attention heads, 1 key-value head, a model dimension of 512, a feed-forward dimension of 2048, and a base RoPE frequency of 10,000. Speaker identity is provided by extracting embeddings from the target utterance using WavLM-base (Chen et al., 2022), fine-tuned for speaker verification. The pooled speaker embedding is prepended to the character embeddings to condition the model on speaker identity. The embedding layer is initialized using the discrete embeddings from the codec quantizer. Training is performed using next-token prediction, where the input sequence consists of the pooled speaker embedding, character embeddings, and speech token embeddings. The cross-entropy loss is computed only over speech tokens, while character and speaker embeddings are excluded from the loss. If the codec has multiple codebooks, we flatten tokens across the codebook dimension, as this approach has been shown to provide the best downstream performance given a sufficiently high computational budget (Copet et al., 2023). At inference time, we use top-p sampling with ß1 = 0.9 and a temperature of 1.0. Following Tian et al. (2025); Della Libera et al. (2025a), we generate five samples per utterance and select the one with the lowest word error rate relative to the input text, computed using Whisper-small (Radford et al., 2023).
Non-autoregressive TTS. The non-autoregressive model employs the same Llama 3 architecture, where causal self- attention is replaced with bidirectional self-attention, while all other components remain unchanged. The pooled speaker


embedding is replicated for the number of character positions and summed with the hidden representations at each layer to condition the model on speaker identity. Training is performed using per-character target prediction with a cross-entropy loss at each step. At inference time, decoding is performed using argmax.
E Additional Results
E.1 Decoding Mode
Table 6 analyzes the impact of different decoding modes on LibriSpeech speech resynthesis. Overall, explicitly providing duration information consistently improves intelligibility, while naturalness and speaker similarity remain largely stable across decoding modes. For all DyCAST variants, tokens + durations yields the lowest dWER, confirming that explicit duration supervision enables more accurate temporal reconstruction and reduces linguistic distortions. This effect is particularly pronounced for predicted-duration variants, where dWER increases moderately as duration information is removed, highlighting the importance of duration modeling at low token rates. In contrast, tokens only decoding achieves comparable or slightly higher UTMOS across all variants, indicating that naturalness is largely preserved even without explicit timing cues. However, this comes at the cost of degraded intelligibility, especially for aggressive compression regimes, where the absence of duration constraints leads to temporal misalignment and increased recognition errors. The intermediate tokens + utterance length mode offers a favorable trade-off, recovering most of the intelligibility gains of full duration decoding while requiring only a global length constraint. Notably, this mode performs robustly across all variants and is used as the default setting in our experiments unless otherwise stated. Overall, these results show that DyCAST enables flexible decoding strategies with explicit control over the trade-off between timing accuracy, intelligibility, and decoding complexity, further emphasizing the practical advantages of variable-rate tokenization with explicit duration modeling.
Table 6. Effect of different decoding modes on LibriSpeech resynthesis.


DyCAST-CATokens + durations Tokens + utterance length3.85
3.993.07
3.3297.5
97.4Tokens only4.033.4197.4Tokens + durations
DyCAST-BP1 Tokens + utterance length Tokens only3.94	2.37	97.7
3.92	3.61	97.6
3.93	4.49	97.6
Tokens + durations
DyCAST-BP3 Tokens + utterance length Tokens only
Tokens + durations
DyCAST-BP5 Tokens + utterance length Tokens only

3.95	2.99	97.4
3.95	4.66	97.2
3.96	5.13	97.2
3.93	5.12	96.6
3.97	8.84	96.5
3.98	8.67	96.5


E.2 Qualitative Analysis of DyCAST Chunk Boundaries
Figure 3 provides qualitative spectrogram visualizations of DyCAST chunk boundaries for the four variants considered in this work: DyCAST-CA, DyCAST-BP1, DyCAST-BP3, and DyCAST-BP5. DyCAST-CA relies on boundaries provided by the character aligner, while the remaining variants use the boundary predictor with min gap set to 1, 3, and 5, respectively, resulting in average frame rates of approximately 14 Hz, 17 Hz, 9 Hz, and 6 Hz. As the frame rate decreases, chunk boundaries become less frequent and more widely spaced, reflecting longer token durations and coarser temporal resolution.
DyCAST-CA	DyCAST-BP1	DyCAST-BP3	DyCAST-BP5
Figure 3. DyCAST chunk boundaries at average frame rates of approximately 14 Hz, 17 Hz, 9 Hz, and 6 Hz (from left to right).














